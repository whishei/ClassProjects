{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Pre-trained Language Models: SubTask C\n",
    "\n",
    "In this assignment, you will work on the [ComVE](https://competitions.codalab.org/competitions/21080) shared task that was part of SemEval-2020. The task aims to evaluate whether a system can distinguish if a natural language statement makes sense to humans or not and provide a reason. **ConVE** includes three subtasks that require models to acquire and apply commonsense knowledge. In this notebook you will focus on **SubTask C**:\n",
    "\n",
    "- Given a statement that does not make sense, generate the reason why this statement does not make sense. For each nonsensical statement, three valid reasons are given as reference:\n",
    "\n",
    "     *Statement*: He put an elephant into the fridge.  \n",
    "     *Reason A*: An elephant is much bigger than a fridge.  \n",
    "     *Reason B*: A fridge is much smaller than an elephant.  \n",
    "     *Reason C*: Most of the fridges aren't large enough to contain an elephant.\n",
    "\n",
    "     This subtask can be approached as a Sequence-to-Sequence problem where the input is the nonsensical statement and the output is a valid reason.\n",
    "\n",
    "You will fine-tune a Pre-trained Language Model with [Transformers](https://huggingface.co/docs/transformers/index) library that provides a set of tools for fine-tunning and deploying a wide variety of Pre-trained Language Models. The [Hugging Face Hub](https://huggingface.co/models) allows you to explore all the models supported by **Transformers** and even share your own models with the community. In this assignment, you will work with [BART](https://huggingface.co/docs/transformers/model_doc/bart), a pre-trained Sequence-to-Sequence model.\n",
    "\n",
    "Fine-tuning a Pre-trained Language Model usually requires a great amount of time and computational resources. Your personal computer will not be enough. In order to complete the assignment, you can work with a reduced version of the dataset and the base version of **BART**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shrink_dataset = True\n",
    "base_model = True\n",
    "colab = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Although the value of these variables do not affect the tests that will evaluate your code, the output examples distributed throughout this notebook are based on a `shrink_dataset` and a `base_model` variables set as `True`, and a `colab` variable set as `False`.\n",
    "\n",
    "If you want to perform a full training of the model to obtain its real performance, you can use a cloud service like [Google Colab](https://colab.research.google.com/). **Colab** is a **Jupyter** notebook environment that supports both GPU and TPU instances, allowing training large scale Deep Learning models. Set the `shrink_dataset` and a `base_model` variables to `False`, the `colab` variable to `True`, and follow the instructions provided to you to run the notebook in **Colab**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    ! pip install transformers datasets evaluate\n",
    "    import os\n",
    "    if not os.path.exists(\"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"):\n",
    "        ! git clone https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.git SemEval2020-Task4-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You will use the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer, \n",
    "                          DataCollatorForSeq2Seq, enable_full_determinism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "When working with Neural Networks, there are a large number of random operations such as initializing the weights of the network, shuffling the data for training, or choosing samples. This causes that different training runs of the same model can lead to different results. To ensure reproducibility, i.e. obtaining the same results in the different runs, the random number generator must be initialized with a fixed value known as seed. In Transformers, this can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enable_full_determinism(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> **Note!** With models as complex as Neural Networks, reproducibility is susceptible to factors such as software versions or the hardware on which the models are run. Even with seed initialization, there may be slight differences in the results.\n",
    "\n",
    "Working with Neural Networks also involves defining a number of hyperparameters that set the configuration of the model. Finding the appropriate hyperparameter values requires training the model with different combinations and testing them on the development set. This hyperparameter tuning is a costly process that needs multiple rounds of experimentation. However, for this assignments, you will use the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 3  # Number of epochs to train the model\n",
    "batch_size = 8  # Number of examples used per gradient update\n",
    "learning_rate = 1e-5  # The learning rate for the optimizer\n",
    "max_length = 25  # Maximum lenght of the input sequence\n",
    "output_dir = \"modelC\"  # The output directory where the model will be written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "> **Note!** The notebook for this assignment provides very little guidance. You are expected to refer to the [documentation](https://huggingface.co/docs) for details on how to solve the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Loading the Pre-trained Model\n",
    "\n",
    "The first step you must perform in this assignment is to load the model and its corresponding tokenizer using the classes imported above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\" if base_model else \"facebook/bart-large\"\n",
    "model, tokenizer = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "The **ComVE** dataset consists of 10000 nonsensical statements for the train set, 997 statements for development and 1000 for test. Each nonsensical statements comes with with three reference valid reasons. You must load the three sets into three `DataFrames`. For the training and development splits, the `DataFrame` should contain three columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and a `reason` column with the reference reasons. For the test set, the `DataFrame` should contain five columns: the `id` of the nonsensical statement, a `FalseSent` column with the nonsensical statement and three columns (`reason1`, `reason2` and `reason3`) containing each of the reference reasons.\n",
    "\n",
    "Train DataFrame:\n",
    "\n",
    "|       |   id | FalseSent                                         | reason                                                                         |\n",
    "|------:|-----:|:--------------------------------------------------|:-------------------------------------------------------------------------------|\n",
    "|   769 |  769 | Computers is an ingredient used in preparing food | Computers are not used for food and they are not edible                        |\n",
    "| 10769 |  769 | Computers is an ingredient used in preparing food | Computer is not something that can be used in preparing food.                  |\n",
    "| 20769 |  769 | Computers is an ingredient used in preparing food | You cannot eat a computer                                                      |\n",
    "|   888 |  888 | he did hear music in his cooling glass            | cooling glass can not play the song, it's not a electronic thing to play music |\n",
    "| 10888 |  888 | he did hear music in his cooling glass            | Glass does not produce music.                                                  |\n",
    "| 20888 |  888 | he did hear music in his cooling glass            | Any sound that might be made by a cooling glass is not music.                  |\n",
    "\n",
    "Test DataFrame:\n",
    "\n",
    "|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            |\n",
    "|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|\n",
    "|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      |\n",
    "| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   |\n",
    "| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. |\n",
    "| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        |\n",
    "| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 |\n",
    "| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_csv, answers_csv, is_test=False):\n",
    "    data = pd.read_csv(data_csv).dropna()\n",
    "    answers = pd.read_csv(answers_csv, header=None).rename(columns={0: \"id\", 1: \"reason1\",\\\n",
    "                                                                      2: \"reason2\", 3: \"reason3\"})\n",
    "    if is_test == True:\n",
    "        merged = pd.merge(data,answers,on=\"id\")\n",
    "    \n",
    "    if is_test == False:\n",
    "        answers1 = answers.drop(['reason2', 'reason3'], axis=1)\n",
    "        answers1 = answers1.rename(columns={'reason1':'reason'})\n",
    "        merged1 = pd.merge(data,answers1,on=\"id\")\n",
    "        answers2 = answers.drop(['reason1', 'reason3'], axis=1)\n",
    "        answers2 = answers2.rename(columns={'reason2':'reason'})\n",
    "        merged2 = pd.merge(data,answers2,on=\"id\")\n",
    "        answers3 = answers.drop(['reason1', 'reason2'], axis=1)\n",
    "        answers3 = answers3.rename(columns={'reason3':'reason'})\n",
    "        merged3 = pd.merge(data,answers3,on=\"id\")\n",
    "        merged = pd.concat([merged1,merged2], ignore_index=True)\n",
    "        merged = pd.concat([merged,merged3], ignore_index=True)\n",
    "        #merged = merged.sort_values(by = ['id'], ascending = [True])\n",
    "        #merged = merged.sort_index()\n",
    "        merged = merged.rename_axis('MyID').sort_values(by = ['id', 'MyID'], ascending = [True, True])\n",
    "        merged.index.name = None\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_data_all.csv\"\n",
    "train_answers_csv = \"SemEval2020-Task4-Data/ALL data/Training  Data/subtaskC_answers_all.csv\"\n",
    "train_data = load_data(train_data_csv, train_answers_csv)\n",
    "dev_data_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_dev_data.csv\"\n",
    "dev_answers_csv = \"SemEval2020-Task4-Data/ALL data/Dev Data/subtaskC_gold_answers.csv\"\n",
    "dev_data = load_data(dev_data_csv, dev_answers_csv)\n",
    "test_data_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_test_data.csv\"\n",
    "test_answers_csv = \"SemEval2020-Task4-Data/ALL data/Test Data/subtaskC_gold_answers.csv\"\n",
    "test_data = load_data(test_data_csv, test_answers_csv, True)\n",
    "if shrink_dataset:\n",
    "    idxs = train_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    train_data = train_data[train_data.id.isin(idxs)]\n",
    "    idxs = dev_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    dev_data = dev_data[dev_data.id.isin(idxs)]\n",
    "    idxs = test_data[\"id\"].sample(frac=1, random_state=42).unique()[:30]\n",
    "    test_data = test_data[test_data.id.isin(idxs)]\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(\"Train DataFrame:\")\n",
    "display(train_data[:6])\n",
    "print(\"Test DataFrame:\")\n",
    "display(test_data[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "print(\"Train Dataset example:\")\n",
    "display(train_dataset[0])\n",
    "print(\"Test Dataset example:\")\n",
    "display(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The `Datasets` should be pre-processed following two different approaches. For the test `Dataset`, you must run the tokenizer on the `FalseSent` column and store the result in the `input_ids` and `attention_mask` fields. For the train and development `Datasets` you must also run the tokenizer on the `reason` column and store the resulting `input_ids` in the `labels` field.\n",
    "\n",
    "><pre>\n",
    ">Train formated Dataset example:\n",
    ">\n",
    ">{'id': 769, 'FalseSent': 'Computers is an ingredient used in preparing food', 'reason': 'Computers are not used for food and they are not edible', '__index_level_0__': 769, 'input_ids': [0, 14721, 43990, 16, 41, 16181, 341, 11, 4568, 689, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 14721, 43990, 32, 45, 341, 13, 689, 8, 51, 32, 45, 27532, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    ">\n",
    ">Test formated Dataset example:\n",
    ">\n",
    ">{'id': 1280, 'FalseSent': 'Beer that is drunk by humans is white', 'reason1': 'Beer is made of barley and it is a yellow drink', 'reason2': 'A beer that is drunk by humans is not white.', 'reason3': 'Beer is brown', '__index_level_0__': 76, 'input_ids': [0, 45562, 14, 16, 10789, 30, 5868, 16, 1104, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, max_length, is_test=False):\n",
    "    if is_test == True:\n",
    "        encoded_indexs = tokenizer(examples['FalseSent'], padding = 'max_length', truncation = True, max_length = max_length)\n",
    "                                   #padding = 'max_length', \n",
    "                                   #max_length = max_length)\n",
    "    else:\n",
    "        '''\n",
    "        encoded_indexs = tokenizer(examples['FalseSent'], examples['reason'],padding = 'max_length', \\\n",
    "                                   max_length = max_length)\n",
    "        '''\n",
    "        encoded_indexs = tokenizer(examples['FalseSent'], padding = 'max_length', truncation = True, max_length = max_length)\n",
    "                                   #padding = 'max_length', \n",
    "                                   #max_length = max_length)\n",
    "        new_encoded = tokenizer(examples['reason'], padding = 'max_length', truncation = True, max_length = max_length)\n",
    "                                    #padding = 'max_length', \n",
    "                                   #max_length = max_length)\n",
    "        encoded_indexs['labels'] = new_encoded['input_ids']\n",
    "       # print (max_length)\n",
    "    return encoded_indexs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "dev_dataset = dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length, True), batched=True)\n",
    "print(\"Train formated Dataset example:\\n\")\n",
    "print(train_dataset[0])\n",
    "print(tokenizer.convert_ids_to_tokens(train_dataset[0][\"input_ids\"]))\n",
    "print(\"\\nTest formated Dataset example:\\n\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "In general, when using a `Trainer` to make predictions, it returns the logits for each class in the task. However, the `Seq2SeqTrainingArguments` class provides an option that allows the `Trainer` to generate sequences of tokens in the prediction. The `create_training_arguments` function must create the `Seq2SeqTrainingArguments` with that option and the hyperparamters passed as arguments. During the training, the model must be evaluated on the development set after every epoch. `TrainingArguments` should include this strategy.\n",
    "\n",
    "> **Important!** By default, `Trainer` saves a checkpoint of the model every 500 training steps. For this assignment, avoid this behavior by setting `save_strategy=\"no\"` when creating the `TrainingArguments`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_training_arguments(epochs, batch_size, learning_rate, output_dir):\n",
    "    training_args = Seq2SeqTrainingArguments(save_strategy = \"no\",do_eval = True,output_dir = output_dir, evaluation_strategy=\"epoch\", \\\n",
    "                                      learning_rate = learning_rate, per_device_train_batch_size = batch_size, \\\n",
    "                                      num_train_epochs = epochs,predict_with_generate = True)\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_args = create_training_arguments(epochs, batch_size, learning_rate, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Next, you can create a `Trainer` object initializing the appropriate data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer):\n",
    "    trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model = model),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, train_args, train_dataset, dev_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If you have set the `Seq2SeqTrainingArguments` properly, you could now use the `Trainer` to predict sequences of tokens. Take into account that `Trainer` will return the indexes of the tokens, so the sequence must be decoded to obtain the text strings. The `tokenizer` provides functionality to do this. The result of this process can be stored in the `prediction` column of the test `DataFrame`:\n",
    "\n",
    "|     |   id | FalseSent                                      | reason1                                                  | reason2                                                | reason3                                                            | prediction                                     |\n",
    "|----:|-----:|:-----------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------|\n",
    "|  76 | 1280 | Beer that is drunk by humans is white          | Beer is made of barley and it is a yellow drink          | A beer that is drunk by humans is not white.           | Beer is brown                                                      | Beer is not white.                             |\n",
    "| 101 |  860 | eating trash food every day makes you stronger | eating trash food every day makes your body fat and weak | eating trash food every day is bad for your health     | Trash food could be contaminated                                   | eating trash food every day makes you stronger |\n",
    "| 136 |  777 | he put some cooking oil in his wine            | cooking oil will destroy the taste of the wine           | Cooking oil does not go in wine                        | Cooking oil does not taste nice and therefore would ruin the wine. | he put some cooking oil in his wine            |\n",
    "| 174 |  570 | Lobsters live in the mountains                 | Lobsters needs water to live                             | Lobsters live in the sea.                              | Lobsters live in the sea, not the mountains                        | Lobsters live in the mountains                 |\n",
    "| 210 | 1929 | the clock shows animals                        | the clock is used to show the time to people             | Clocks are required to tell the time, not show animals | a clock shows the time not animals                                 | the clock shows animals                        |\n",
    "| 235 | 1619 | she put the giraffe in the freezer             | A giraffe is much bigger than the freezer                | There is no way a giraffe is fitting in the freezer.   | A giraffe is too big to be put in a freezer.                       | she put the giraffe in the freezer             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(trainer, test_dataset, tokenizer):\n",
    "    predictions,labels,metrics = trainer.predict(test_dataset)\n",
    "    #print (labels)\n",
    "    #predictions = predictions.argmax(axis=-1)\n",
    "    #predictions = tokenizer.decode([predictions])\n",
    "    #sentence = \"\"\n",
    "    new_tokens = []\n",
    "    for token in predictions:\n",
    "        new_sent = tokenizer.decode(token, skip_special_tokens = True)\n",
    "        new_tokens.append(new_sent)\n",
    "    #for token in predictions[0]:\n",
    "    #    word = tokenizer.decode([token], skip_special_tokens = True)\n",
    "        #new_sentence = text.replace(tokenizer.pre, word)\n",
    "    '''\n",
    "    if word in tokenizer.vocab:\n",
    "        sentence = sentence + str(word)\n",
    "    '''\n",
    "    #    print(word)\n",
    "    #print (tokenizer.vocab)\n",
    "    #predictions = tokenizer.vocab[predictions] \n",
    "    return new_tokens#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = make_predictions(trainer, test_dataset, tokenizer)\n",
    "test_data[\"prediction\"] = predictions\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The **Subtasks B** of **ComVE** is evaluated using the *bleu* metric. In this assignment, you will also evaluate using *rouge*. With `shrink_dataset` and `base_model` set to `True`, the expected scores are *0.216* and *0.446* for *bleu* and *rouge* respectively. With a full training run, i.e. with `shrink_dataset` and `base_model` set to `False`, the scores should be around *0.228* and *0.461*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_prediction(test_data, metric):\n",
    "    \n",
    "    refs = []\n",
    "    refs1 = list(test_data[\"reason1\"].values)\n",
    "    refs2 = list(test_data[\"reason2\"].values)\n",
    "    refs3 = list(test_data[\"reason3\"].values)\n",
    "    for i in range(0,len(refs1)):\n",
    "        inner = []\n",
    "        inner.append(refs1[i])\n",
    "        inner.append(refs2[i])\n",
    "        inner.append(refs3[i])\n",
    "        #rint (inner)\n",
    "        refs.append(inner)\n",
    "    #rint (refs)\n",
    "    #print (refs)\n",
    "    accuracy = evaluate.load(metric)\n",
    "    #reasons\n",
    "    return accuracy.compute(predictions=test_data[\"prediction\"].values, references = refs)#references=test_data[\"FalseSent\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " evaluate_prediction(test_data, \"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_prediction(test_data, \"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The scores for the partial training and the full training are so similar that it would appear that the full training does not provide any benefit in this task. However, it should be noted that the test sets in the two cases are different. More importantly, these results are indicative of the limitations of metrics such as *bleu* and *rouge* for evaluating text generation. Take, for example, the following case from the test set:\n",
    "\n",
    "\n",
    "| FalseSent                 | reason1                                        | reason2                          | reason3                         |\n",
    "|:--------------------------|:-----------------------------------------------|:---------------------------------|:--------------------------------|\n",
    "| Beer that is drunk by humans is white | Beer is made of barley and it is a yellow drink | A beer that is drunk by humans is not white. | Beer is brown |\n",
    "\n",
    "The predictions obtained by the partial and full trainings and their corresponding scores are the following:\n",
    "\n",
    "| full training    | prediction                 | bleu     | rouge    |\n",
    "|:-----------------|:---------------------------|---------:|---------:|\n",
    "| no               | Beer that is drunk by humans is white  | 0.731    | 0.889    |\n",
    "| yes              | White beer is not suitable for human consumption. | 0.000    | 0.364    |\n",
    "\n",
    "The text generated by the full training is a better explanation than the reason generated by the partial training, which is a mere repetition of the nonsensical statement. However, the latter obtains much better scores than the former. Metrics such as *bleu* and *rouge* do not always replace accurately the human judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
