{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Text Acquisition and Pre-processing\n",
    "\n",
    "In this assignment you will practice obtaining, extracting, cleaning and pre-processing text from an online source. The objective is to obtain the text from a web page and generate a **pandas** DataFrame containing the text segmented, tokenized and with different types of linguistic annotations.\n",
    "\n",
    "You will work with the following objects and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Text Extraction\n",
    "\n",
    "The text you are going to work with corresponds to the following post from the Food and Agriculture Organization of the United Nations website: [World food prices dip in December](https://www.fao.org/newsroom/detail/world-food-prices-dip-in-december/en).\n",
    "\n",
    "In a more realistic scenario, you should download the html document yourself. This could be done with the following code snippet:\n",
    "\n",
    ">```python\n",
    "import requests\n",
    "URL = \"https://www.fao.org/newsroom/detail/world-food-prices-dip-in-december/en\"\n",
    "page = requests.get(URL)\n",
    "html_content = page.content\n",
    "\n",
    "However, for this assignment, you are provided with the downloaded document. The file`world-food-prices.html` can be found in the same directory as this notebook and it can be opened as a regular text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "with open(\"world-food-prices.html\", encoding=\"utf8\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "html_content[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see the document contains a lot of html tags as well as some **javascript** code. The text also includes fields that are not of interest, such as the navigation menu of the web page. The goal of the first step in this assignment is to extract only the text from the body of the post.\n",
    "\n",
    "To do this, you must complete the code for the `extract_text` function. This function should parse the content of the html document using the **BeatifulSoup** library, find the html element containing the text of the body of the post, and extract such text. The body of the post is contained by the element with the following **id**: `\"Contentplaceholder1_C011_Col00\"`. Review the [BeautifullSoup documentation](https://beautiful-soup-4.readthedocs.io/en/latest/index.html) to learn how to perform these steps.\n",
    "\n",
    "\n",
    "The function must return the text extracted of which the first 580 characters should look like this:\n",
    "\n",
    "\n",
    "><pre>'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorld food prices dip in December\\nFAO Food Price Index ends 2022 lower than a year earlier\\n\\n\\n\\n\\n                                A farmer in Sicily carrying wheat seeds.\\n                             \\n\\n©FAO/Giorgio Cosulich \\n\\n\\n\\n\\n06/01/2023\\n\\n\\nRome – The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. The FAO Food Price Index averaged 132.4 points in December, 1.0 percent below its value a year earlier. '</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    body = soup.find(id = \"Contentplaceholder1_C011_Col00\")\n",
    "    return (body.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = extract_text(html_content)\n",
    "text[:580]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Text Cleanup\n",
    "\n",
    "The text extracted by `extract_text` is not still ready to use. It contains several newline characters and additional spaces that make the text noisy. In the next step of the assignment, you must complete the code for the function `clean_text`. The function should take the text and delete all those newline characters and extra blank spaces. The function should also add a period to the end of those sentences that do not originally contain it, for example, `World food prices dip in December` or `06/01/2023`.\n",
    "\n",
    "You can solve this exercise using the **Python** built-in [string methods](https://docs.python.org/3.9/library/stdtypes.html?highlight=replace#str), such as `replace`, or by [regular expressions](https://docs.python.org/3.9/library/re.html?highlight=re#module-re).\n",
    "\n",
    "The `extract_text` function must return the cleaned text of which the first 500 characters should look like this:\n",
    "\n",
    ">'World food prices dip in December. FAO Food Price Index ends 2022 lower than a year earlier. A farmer in Sicily carrying wheat seeds. ©FAO/Giorgio Cosulich. 06/01/2023. Rome – The index of world food prices dipped for the ninth consecutive month in December 2022, declining by 1.9 percent from the previous month, the Food and Agriculture Organization of the United Nations (FAO) reported today. The FAO Food Price Index averaged 132.4 points in December, 1.0 percent below its value a year earlier. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Split by '\\n' characters\n",
    "    new_text = text.split('\\n')\n",
    "    #Remove all leading and trailing white space\n",
    "    new_text = [word.strip() for word in new_text]\n",
    "    #Remove all empty strings\n",
    "    new_text = [i for i in new_text if i]\n",
    "    #Remove periods from end of sentence if already there\n",
    "    new_text = [word.strip('.') for word in new_text]\n",
    "    #Join the text back together with '. ' seperating each sentence\n",
    "    new_text = '. '.join(new_text)\n",
    "    #Add last period to final sentence\n",
    "    new_text = new_text + '.'\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "cleaned_text = clean_text(text)\n",
    "cleaned_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "Once the text has been extracted and cleaned up, the next step you must take is to pre-process it. For this, in this assignment, you are going to use the [spaCy](https://spacy.io/) library. This library is an advanced NLP toolkit that allows to execute various pre-processing steps as well as different NLP tasks. **spaCy** provides trained [pipelines](https://spacy.io/usage/processing-pipelines) for a variety of languages that can be installed as individual **Python** modules and include [linguistic featues](https://spacy.io/usage/linguistic-features) such as:\n",
    "\n",
    "- Sentence Segmentation\n",
    "- Tokenization\n",
    "- Stemming and Lemmatization\n",
    "- Stopwords\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Named Entity Recognition\n",
    "- Word Embeddings\n",
    "\n",
    "In this exercise, you will work with the [English pipeline optimized for CPU](https://spacy.io/models/en#en_core_web_sm) that can be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You must complete the code for the `preprocess_text` function. This function takes the text and a **spaCy** pipeline as input and should run that pipeline on the text. The function must return a [Doc](https://spacy.io/api/doc) object. Check the [spaCy 101](https://spacy.io/usage/spacy-101) documentation to learn how to apply the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, nlp):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "doc = process_text(cleaned_text, nlp)\n",
    "all(map(doc.has_annotation, [\"LEMMA\", \"POS\", \"ENT_TYPE\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Creating a DataFrame\n",
    "\n",
    "In the next exercise, you will create a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) that will contain some of the linguistic annotations from the `Doc` object obtained in the previous step. Loading the data into a `DataFrame` provides some advantages such as a better integration with other **Python** machine learning libraries or the option to save the data in a csv file.\n",
    "\n",
    "The goal is to create a `DataFrame` that contains a row per each token in the `Doc` and the following columns:\n",
    "- *sent_id*: The id of the sentence the token belongs to. It represents the position of the sentence in the `Doc`, starting by 0.\n",
    "- *token_id*: The id of the token. It represents the position of the token in the sentence, starting by 0.\n",
    "- *text*: The original text of the token.\n",
    "- *lemma*: The lemmatization of the token.\n",
    "- *pos*: The part-of-speech of the token.\n",
    "- *ent*: The entity type of the token returned by the Named Entity Recognition component.\n",
    "\n",
    "You must complete the code for the `to_dataframe` function. This function takes the [Doc](https://spacy.io/api/doc) object and must return the `DataFrame` described above. The function should iterate over the sentences in the `Doc` (each sentence is a [Span](https://spacy.io/api/span) object) and, for each sentence, it should iterate over its tokens (each token is a [Token](https://spacy.io/api/token) object). For each token, `to_dataframe` should obtain the values to fill the *text*, *lemma*, *pos* and *ent* columns of the `DataFrame`. For example, the content of the `DataFrame` for the setence with *sent_id* equal to 1, corresponding to the second sentence in the `Doc`, should look like this:\n",
    "\n",
    "|    |   sent_id |   token_id | text    | lemma   | pos   | ent   |\n",
    "|---:|----------:|-----------:|:--------|:--------|:------|:------|\n",
    "|  7 |         1 |          0 | FAO     | FAO     | PROPN | ORG   |\n",
    "|  8 |         1 |          1 | Food    | Food    | PROPN | ORG   |\n",
    "|  9 |         1 |          2 | Price   | Price   | PROPN | ORG   |\n",
    "| 10 |         1 |          3 | Index   | Index   | PROPN | ORG   |\n",
    "| 11 |         1 |          4 | ends    | end     | VERB  |       |\n",
    "| 12 |         1 |          5 | 2022    | 2022    | NUM   | DATE  |\n",
    "| 13 |         1 |          6 | lower   | low     | ADJ   |       |\n",
    "| 14 |         1 |          7 | than    | than    | ADP   |       |\n",
    "| 15 |         1 |          8 | a       | a       | DET   | DATE  |\n",
    "| 16 |         1 |          9 | year    | year    | NOUN  | DATE  |\n",
    "| 17 |         1 |         10 | earlier | early   | ADV   | DATE  |\n",
    "| 18 |         1 |         11 | .       | .       | PUNCT |       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(doc):\n",
    "    #Create an empty pandas dataframe with the column labels provided\n",
    "    df = pd.DataFrame(columns = ['sent_id','token_id','text','lemma','pos','ent'])\n",
    "    #Obtain the sentences (span objects) from the document\n",
    "    spans = doc.sents \n",
    "    #Iterate through the sentences (spans) of the document\n",
    "    for idx,span in enumerate(spans):\n",
    "        #Iterate through the words (tokens) of each sentence (span)\n",
    "        for ind,token in enumerate(span):\n",
    "            #Append each token and labels to the dataframe as an entry\n",
    "            df = df.append({'sent_id':idx, 'token_id':ind, 'text':token.text, 'lemma':token.lemma_, 'pos':token.pos_, 'ent':token.ent_type_}, ignore_index = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "df = to_dataframe(doc)\n",
    "df[df.sent_id == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Cutomizing the Tokenizer\n",
    "\n",
    "The default components of a **spaCy** pipeline will not always behave according to the needs of your projects. For example, the default tokenizer of the `en_core_web_sm` pipeline does not always splits dates in `month/day/year` format into `month`, `day` and `year`. This is the case for the sentence with *sent_id* equal to 4 that only includes a date in that format:\n",
    "\n",
    "|    |   sent_id |   token_id | text       | lemma      | pos   | ent   |\n",
    "|---:|----------:|-----------:|:-----------|:-----------|:------|:------|\n",
    "| 32 |         4 |          0 | 06/01/2023 | 06/01/2023 | NUM   |       |\n",
    "| 33 |         4 |          1 | .          | .          | PUNCT |       |\n",
    "\n",
    "The goal of the last exercise of this task is to update the `en_core_web_sm` pipeline with a custom tokenizer that forces the splitting of dates in `month/day/year` format so that the sentence above looks like this:\n",
    "\n",
    "|    |   sent_id |   token_id | text   | lemma   | pos   | ent      |\n",
    "|---:|----------:|-----------:|:-------|:--------|:------|:---------|\n",
    "| 32 |         4 |          0 | 06     | 06      | NUM   | CARDINAL |\n",
    "| 33 |         4 |          1 | /      | /       | SYM   |          |\n",
    "| 34 |         4 |          2 | 01     | 01      | NUM   |          |\n",
    "| 35 |         4 |          3 | /      | /       | SYM   |          |\n",
    "| 36 |         4 |          4 | 2023   | 2023    | NUM   |          |\n",
    "| 37 |         4 |          5 | .      | .       | PUNCT |          |\n",
    "\n",
    "You must complete the code for the `customize_tokenizer` function. The function takes the **spaCy** pipeline as input. It should create the customized tokenizer and return the updated version of the pipeline including the customized tokenizer. The `Tokenizer` must be created with the default vocabulary and the default prefixes and suffixes rules of the pipeline. You should only update the infixes rules adding a regular expression that captures slash (`/`) characters. The `Tokenizer` should **not** include special cases or rules for token and url matching. Check the [spacy's documentation](https://spacy.io/usage/linguistic-features#native-tokenizers) to learn how to customize the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_tokenizer(nlp):\n",
    "    infixes = nlp.Defaults.infixes + [r'''/''',]\n",
    "    infix_regex = spacy.util.compile_infix_regex(infixes)\n",
    "    nlp.tokenizer.infix_finditer = infix_regex.finditer\n",
    "    return nlp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "customized_nlp = customize_tokenizer(nlp)\n",
    "doc = process_text(cleaned_text, customized_nlp)\n",
    "df = to_dataframe(doc)\n",
    "df[df.sent_id == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
